# ğŸ“Š Predictive Modeling with Inside Airbnb

[![Python](https://img.shields.io/badge/python-3.10-blue)](https://www.python.org/)
[![Status](https://img.shields.io/badge/status-active-brightgreen)]()
[![License](https://img.shields.io/badge/license-MIT-lightgrey)]()

An end-to-end data science pipeline using the **Inside Airbnb** `listings.csv` to predict listing prices (regression), Superhost status (classification), and build an advanced XGBoost probability model to identify high-potential non-Superhosts.

---

## ğŸ”­ Table of contents

* [Project overview](#project-overview)
* [Goals](#goals)
* [Tech stack](#tech-stack)
* [Project structure](#project-structure)
* [Setup & installation](#setup--installation)
* [ğŸƒ Usage](#-usage)

  * [Run main pipeline (Tasks 1â€“4)](#run-main-pipeline-tasks-1-4)
  * [Run advanced XGBoost analysis (Task 5)](#run-advanced-xgboost-analysis-task-5)
* [ğŸ”¬ Notebook summaries](#-notebook-summaries)
* [ğŸ›  Key techniques & methods](#-key-techniques--methods)
* [âœ… Outputs / expected files](#-outputs--expected-files)
* [ğŸ§­ Tips & GitHub Markdown hints](#-tips--github-markdown-hints)
* [Contributing](#contributing)
* [License](#license)

---

## ğŸ“ Project overview

This repository contains reproducible notebooks and scripts that implement:

* Data cleaning & feature engineering (including sentiment with VADER)
* Price prediction (regression)
* Superhost prediction (classification)
* Advanced XGBoost probability estimation for future Superhosts

---

## ğŸ¯ Goals

* **Task 1 â€” Regression:** Predict listing price using engineered features.
* **Task 2 â€” Classification:** Predict whether a host becomes a Superhost.
* **Task 3 â€” Advanced:** Generate reliable probability scores using XGBoost to identify high-potential non-Superhosts.

> **Note:** Add `listings.csv` from Inside Airbnb before running any scripts.

---

## ğŸ›  Tech stack

* **Core:** pandas, numpy, scikit-learn, matplotlib, seaborn
* **Modeling:** XGBoost, Logistic Regression, Decision Tree, SVM
* **NLP:** NLTK VADER sentiment analyzer
* **Imbalance Handling:** SMOTE, scale_pos_weight
* **Explainability:** SHAP
* **Environment:** Conda, Jupyter Notebooks

---

## ğŸ“ Project structure

```
airbnb-predictive-modeling/
â”‚
â”œâ”€â”€ code/
â”‚   â””â”€â”€ code.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ listings.csv
â”‚   â””â”€â”€ listings_cleaned.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 001_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 002_regression_model.ipynb
â”‚   â”œâ”€â”€ 003_classification_model.ipynb
â”‚   â”œâ”€â”€ 004_model_comparison.ipynb
â”‚   â””â”€â”€ 005_advanced_predictive_probability.ipynb
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_helpers.py
â”‚   â””â”€â”€ preprocessing.py
â”‚
â”œâ”€â”€ visuals/
â”‚   â”œâ”€â”€ 1/
â”‚   â”œâ”€â”€ 2/
â”‚   â”œâ”€â”€ 3/
â”‚   â”œâ”€â”€ 4/
â”‚   â””â”€â”€ 5/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup_structure.py
â””â”€â”€ README.md
```

---

## ğŸš€ Setup & installation

```bash
git clone https://[your-repository-url]/airbnb-predictive-modeling.git
cd airbnb-predictive-modeling

conda create -n airbnb_env python=3.10 -y
conda activate airbnb_env

pip install -r requirements.txt
```

---

## ğŸƒ Usage

### â–¶ï¸ Run main pipeline (Tasks 1â€“4)

Runs preprocessing, regression and classification.

```bash
python main.py
```

#### What this does

* Loads `data/listings.csv`
* Cleans & engineers features â†’ saves `data/listings_cleaned.csv`
* Trains & compares **Regression Models:** Linear, Ridge, Lasso, ElasticNet
* Trains & compares **Classification Models:** Logistic Regression, Decision Tree, SVM, GaussianNB (with SMOTE + tuning)

---

### â–¶ï¸ Run advanced XGBoost analysis (Task 5)

```bash
python code/code.py
```

#### What this does

* Loads `data/listings_cleaned.csv`
* Fixes `5E-1` string corruption in numeric fields
* Trains tuned XGBoost probability model using `scale_pos_weight`
* Generates advanced probability scores
* Identifies **non-Superhosts with >80% likelihood** of becoming Superhosts

---

## ğŸ”¬ Notebook summaries

### ğŸ“˜ 001_data_preprocessing.ipynb

* Read ~79 columns, dropped 30+ irrelevant
* Engineered: `host_duration_days`, `amenities_count`, `description_sentiment`
* Handled missing values & outliers
* Fixed persistent **5E-1 string error**
* Identified severe class imbalance in `host_is_superhost`

### ğŸ“˜ 002_regression_model.ipynb

* Used ColumnTransformer (Scaling + OHE)
* Models: Linear, Ridge, Lasso, ElasticNet
* **Best:** Ridge (RÂ² â‰ˆ 0.70, RMSE â‰ˆ $113)

### ğŸ“˜ 003_classification_model.ipynb

* Baseline model: Logistic Regression
* SMOTE applied via ImbPipeline
* Stratified splitting
* **Results:** Recall â‰ˆ 74%, Precision â‰ˆ 54%

### ğŸ“˜ 004_model_comparison.ipynb

* Compared Decision Tree, SVM, Logistic Regression, GaussianNB
* Hyperparameter tuning with GridSearchCV
* **Best:** Decision Tree (F1 â‰ˆ 0.64)
* **Worst:** GaussianNB (feature correlation issue)

### ğŸ“˜ 005_advanced_predictive_probability.ipynb

* Tuned XGBoost using scale_pos_weight
* Performance: F1 â‰ˆ 0.75, AUC â‰ˆ 0.90
* Found non-Superhosts with perfect review scores but lower response rate

---

## ğŸ›  Key techniques & methods

| Area                | Techniques                                                        |
| ------------------- | ----------------------------------------------------------------- |
| Preprocessing       | ColumnTransformer, OneHotEncoder, StandardScaler, Pipeline        |
| Feature Engineering | VADER Sentiment, Date parsing, Amenities counts, Outlier handling |
| Regression          | Linear, Ridge, Lasso, ElasticNet                                  |
| Classification      | LogisticRegression, DecisionTree, SVM, GaussianNB, XGBoost        |
| Imbalance Handling  | SMOTE, scale_pos_weight, Stratified split                         |
| Tuning              | GridSearchCV                                                      |
| Explainability      | SHAP                                                              |

---

## âœ… Outputs / expected files

* `data/listings_cleaned.csv` â€” cleaned dataset
* `visuals/` â€” plots generated during notebook analysis
* `notebooks/` â€” full experimentation history

---

## ğŸ§­ Tips & GitHub Markdown hints

> **TIP:** If README appears collapsed, ensure you are editing the root `README.md` file.
>
> **TIP:** Use relative links such as:
> `[Preprocessing Notebook](notebooks/001_data_preprocessing.ipynb)`

**Common Markdown elements used here:**

* Headings â†’ `#`, `##`, `###`
* Code blocks â†’ triple backticks
* Inline code â†’ `like this`
* Task lists â†’ `- [ ] item`
* Links â†’ `[text](url)`
* Images â†’ `![alt](path/img.png)`

---

## ğŸ¤ Contributing

Pull requests are welcome! Please follow standard GitHub workflow:

* Fork the repo
* Create a new branch
* Commit changes
* Open a PR

---

## ğŸ“ License

This project is licensed under the **MIT License**.

---

